# rustsh_microservices
rustsh microservices repository

## Домашнее задание № 14 (Технология контейнеризации. Введение в Docker)

Что сделано:
1. Настроена интеграция репозитория с Travis CI по аналогии с репозиторием infra, а также интеграция репозитория на GitHub и Travis CI с каналом в Slack.
2. Поднята виртуальная машина с Linux.
3. На ней установлен Docker, локальный пользователь добавлен в группу docker, чтобы не нужно было каждый раз вводить `sudo`.
4. Исследованы запуск и удаление контейнеров и образов.

## Домашнее задание № 15 (Docker-контейнеры)

Что сделано:
1. Создан новый проект в GCP, установлен GCloud SDK, проект инициализирован и к нему настроен доступ.
2. Установлен docker-machine.
3. Создан docker-хост, окружение переключено на него командой `eval $(docker-machine env docker-host)`.
4. Созданы файлы для формирования docker-образа, включая Dockerfile.
5. Командой `docker build -t reddit:latest .` создан образ приложения reddit.
6. Командой `docker run --name reddit -d --network=host reddit:latest` из нового образа создан и запущен контейнер на VM в GCP.
7. В GCP создано правило файервола для порта 9292, проверена работоспобность приложения reddit.
8. Зарегистрирована учётная запись в Docker Hub, созданный образ загружен в удалённый репозиторий, затем выгружен и проверен локально.

## Домашнее задание № 16 (Docker-образы. Микросервисы)

Что сделано:
1. Скачанный каталог reddit-microservices переименован в src, скачан образ MongoDB.
2. Внутри каждой подпапки src создан и оптимизирован Dockerfile и на его основе создан соответствующий образ каждого компонента.
3. Создана сеть для приложения, внутри неё запущены контейнеры из наших образов с сетевыми алиасами.
4. Создан docker volume.

## Домашнее задание № 17 (Docker: сети, docker-compose)

Что сделано:
1. Контейнеры запущены с разными сетевыми драйверами (none, host, bridge), исследована разница между ними.
2. Исследованы сетевые алиасы и включение контейнеров в несколько сетей.
3. Установлен Docker Compose, создан файл docker-compose.yml с описанием проекта.
4. В docker-compose.yml заданы несколько сетей с их настройками, алиасы, а также параметры, описанные в файле .env.
5. При помощи команды `docker-compose up -d` запущено приложение reddit, состоящее из нескольких компонент.

### Базовое имя проекта
Базовое имя проекта можно задать, используя ключ `-p` (`--project-name`) в команде `docker-compose`. Второй способ — задать это имя в переменной окружения `COMPOSE_PROJECT_NAME` в консоли либо в файле .env.

## Домашнее задание № 18 (Устройство Gitlab CI. Построение процесса непрерывной поставки)

Что сделано:
1. При помощи docker-machine создана VM для GitLab с установленным Docker:
	```bash
	docker-machine create --driver google \
	--google-machine-image https://www.googleapis.com/compute/v1/projects/ubuntu-os-cloud/global/images/family/ubuntu-1604-lts \
	--google-machine-type n1-standard-1 \
	--google-zone europe-west1-b \
	--google-disk-size 50 \
	--google-tags http-server,https-server \
	gitlab-ci 
	```
2. На VM созданы нужные папки, подготовлен файл docker-compose.tml, установлен Docker Compose, затем с его помощью установлен сам GitLab.
3. В GitLab'е создана группа 'homework', внутри которой создан проект 'example'.
4. Добавлен удалённый репозиторий 'gitlab', в котором задана ссылка на GitLab.
5. Создан файл .gitlab-ci.yml для опредения CI/CD пайплайна.
6. На VM запущен и зарегистрирован раннер.
7. В пайплайн добавлено тестирование приложение (и соответствующий скрипт), заданы окружения, новые этапы (staging и production), условия и ограничения, определены динамические окружения.

## Домашнее задание № 19 (Введение в мониторинг. Системы мониторинга)

Что сделано:
1. Созданы правила файервола для Prometheus и Puma, создан Docker хост в GCE и настроено локальное окружение на работу с ним.
2. Запущен контейнер с Prometheus, исследован его интерфейс.
3. Созданы Dockerfile для создания образа с Prometheus и файл конфигурации prometheus.yml.
4. Собраны образы сервисов приложения при помощи скриптов docker_build.sh.
5. В docker-compose.yml опредены новые сервисы (сам Prometheus и Node экспортер).
6. При помощи Docker Compose развернуто приложение с системой мониторинга; исследована работа системы мониторинга.
7. Созданные в ходе работы образы загружены на Docker Hub:
	- ui: https://hub.docker.com/r/rustsh/ui
	- comment: https://hub.docker.com/r/rustsh/comment
	- post: https://hub.docker.com/r/rustsh/post
	- prometheus: https://hub.docker.com/r/rustsh/prometheus

## Домашнее задание № 20 (Мониторинг приложения и инфраструктуры)

Что сделано:
1. Создан Docker хост в GCE и настроено локальное окружение на работу с ним, создано правило файервола для сервисов мониторинга, открыты нужные порты (8080, 3000, 9093).
2. Файл docker-compose.yml разделен на два — для самих приложений (docker-compose.yml) и для мониторинга (docker-compose-monitoring.yml).
3. Установлен и исследован cAdvisor — инструмент для мониторинга состояния docker-контейнеров:
	- сервис добавлен в docker-compose-monitoring.yml;
	- информация о сервисе добавлена в prometheus.yml.
4. Установлена и исследована Grafana — инструмент для визуализации метрик:
	- сервис добавлен в docker-compose-monitoring.yml;
	- в веб-интерфейсе задан источник данных, тип и параметры подключения;
	- загружен и установлен сторонний дашборд;
	- в конфигурацию Prometheus добавлена информацию о сервисе post, чтобы он начал собирать с него метрики;
	- созданы и сохранены собственные дашборды Grafana — в т. ч. для сбора бизнес-метрик.
5. Установлен и исследован Alertmanager — инструмент для оповещения:
	- создан новый Dockerfile для сборки образа alertmanager;
	- создан файл config.yml, в котором определена отправка нотификаций канал Slack, а также создана интеграция с этим каналом;
	- сервис добавлен в docker-compose-monitoring.yml;
	- создан файл alerts.yml, в котором определены условия, при которых алерт должен срабатывать и посылаться в Alertmanager. В Dockerfile для сборки образа Prometheus добавлена операция копирования данного файла в образ;
	- информация о правилах добавлена в prometheus.yml;
	- протестирована отправка оповещений в канал Slack.
6. Созданные в ходе работы образы загружены на Docker Hub: https://hub.docker.com/u/rustsh

## Домашнее задание № 21 (Логирование и распределенная трассировка)

Что сделано:
1. Создан хост logging в GCE и настроено локальное окружение на работу с ним, создано правило файервола для сервисов логирования, открыты нужные порты (9200, 5601, 9411, 24224).
2. Обновлён код приложения из репозитория, на его основе созданы новые образы приложения.
3. Создан отдельный файл docker-compose-logging.yml для системы логирования.
4. Создан Dockerfile для компоновки образа Fluentd, а также файл конфигурации fluent.conf. Внутри compose-файла определён драйвер для логирования для сервисов post и ui.
5. Создан индекс в Kibana, исследован интерфейс.
6. В файл конфигурации fluentd.conf добавлены фильтры для парсинга логов в формате json, приходящих от сервиса post, а также для парсинга неструктурированных логов от ui при помощи регулярных выражений и grok-шаблонов.
7. Установлен и исследован инструмент для распределённого трейсинга Zipkin.
8. Запущенно приложение с ошибкой и выполнено дополнительное задание.

### Задание со :star: — grok
Чтобы распарсить неразобранные логи, в файл fluentd.conf был добавлен следующий фильтр:
```
pattern service=%{WORD:service} \| event=%{WORD:event} \| path=%{URIPATH:path} \| request_id=%{GREEDYDATA:request_id} \| remote_addr=%{IP:remote_addr} \| method= %{WORD:method} \| response_status=%{NUMBER:response_status}
```
Чтобы этот фильтр работал одновременно с уже существующим, необходимо обе строки поместить в тег `<grok>`, а параметр `grok_pattern` заменить на `pattern`.
Таким образом, фрагмент итогового файла конфигурации, отвечающий за одновременный парсинг двух типов неструктурированных логов сервиса ui, имеет следующий вид:
```
<grok>
    pattern service=%{WORD:service} \| event=%{WORD:event} \| request_id=%{GREEDYDATA:request_id} \| message='%{GREEDYDATA:message}'
</grok>
<grok>
    pattern service=%{WORD:service} \| event=%{WORD:event} \| path=%{URIPATH:path} \| request_id=%{GREEDYDATA:request_id} \| remote_addr=%{IP:remote_addr} \| method= %{WORD:method} \| response_status=%{NUMBER:response_status}
</grok>
```

### Задание со :star: — Zipkin
Исследование трейса запроса при помощи Zipkin показало, что практически всё время при открытии любого поста — около 3 секунд — занимает обратка запроса приложением post. При раскрытии подробной информации об этом запросе видно, что это время занимает обращение к странице вида `/post/5cb8eabbf38c62000ef01180`. При анализе кода приложения post_app.py была выявлена причина долгого времени обработки запроса:
```python
max_resp_time = 3
...
median_time = time.sleep(max_resp_time)
```

## Домашнее задание № 22 (Введение в Kubernetes)

Что сделано:
1. Созданы файлы с Deployment-манифестами приложений: post-deployment.yml, ui-deployment.yml, comment-deployment.yml, mongo-deployment.yml.
2. Пройден Kubernetes The Hard Way:
    - в gcloud установлены регион и зона по умолчанию;
    - установлены утилиты cfssl, cfssljson и kubectl;
    - созданы ресурсы облачной инфраструктуры: сеть с подсетью, статический внешний IP-адрес, правила файервола, виртуальные машины — контроллеры и воркеры (по две, а не по три, как в туториале, из-за ограничения количества динамических внешних IP-адресов в пробной версии GCP);
    - поднят локальный удостоверяющий центр и сгенерированы TLS-сертификаты и ключи для следующих компонентов: etcd, kube-apiserver, kube-controller-manager, kube-scheduler, kubelet и kube-proxy, а также для пользователя `admin`. Созданные ключи и сертификаты скопированы на VM;
    - сгенерированы конфигурационные файлы Kubernetes (т. н. kubeconfigs), которые настраивают связь между клиентами и серверным API Kubernetes, для следующих компонентов: controller manager, kubelet, kube-proxy и scheduler, а также для пользователя `admin`. Созданные файлы скопированы на VM;
    - сгенерирован ключ шифрования и конфигурационный файл для шифрования encryption-config.yaml, который передан на VM-контроллеры;
    - на основе VM-контроллеров поднят и настроен кластер хранилища etcd для хранения информации о состоянии компонентов Kubernetes;
    - установлен и настроен Kubernetes control plane, который состоит из следующих компонентов: Kubernetes API Server, Scheduler и Controller Manager. Установлен и настроен nginx для проксирования HTTP-запросов (для работы хелсчеков). Созданы ресурсы GCP для балансировки нагрузки;
    - установлены и настроены рабочие ноды Kubernetes, включающие в себя следующие компоненты: runc, gVisor, container networking plugins, containerd, kubelet и kube-proxy;
    - сгенерирован кофигурационный файл для удаленного доступа утилиты kubectl к кластеру Kubernetes;
    - в GCP для каждой рабочей ноды настроен роутинг, чтобы поды, находящиеся на разных VM, могли взаимодействовать друг с другом;
    - установлен DNS Add-on;
    - протестирована работа кластера: шифрование данных, развёртывание приложений на примере nginx, проброс портов, логирование, выполнение команд внутри контейнера, предоставление сервиса наружу, работа недоверенных приложений под gVisor'ом;
	- в конце работы все созданные ресурсы GCP были удалены.

## Домашнее задание № 23 (Kubernetes. Запуск кластера и приложения. Модель безопасности)

Что сделано:
1. На рабочей машине установлены утилиты kubectl и Minikube.
2. В виде YAML-манифестов описаны ресурсы deployment и service для компонентов приложения reddit.
3. При помощи Minikube в VirtualBox развёрнут локальный кластер Kubernetes, в котором размещены компоненты приложения reddit:
    ```bash
    minikube start
    ...
    kubectl apply -f ./kubernetes/reddit
    ```
4. Исследован графический интерфейс Kubernetes:
    - включен аддон dashboard:
        ```bash
        minikube addons enable dashboard
        ```
    - запущена страница с графическим интерфейсом:
        ```bash
        minikube dashboard
        ```
5. Создан новый namespace `dev`, также описанный в виде yml-файла.
6. В Google Kubernetes Engine создан кластер Kubernetes, информация о нём добавлена в файл `~/.kube/config`.
7. Приложение reddit развёрнуто в кластере GKE в пространстве имён `dev`.
8. Создано правило файервола для доступа к приложению из интернета.
9. В GKE запущен графический интерфейс для кластера:
    - для опции `Kubernetes dashboard` выставлено значение `Enabled`;
    - учётной записи дашборда при помощи привязки назначена роль с достаточными правами на просмотр информации о кластере:
        ```bash
        kubectl create clusterrolebinding kubernetes-dashboard  --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard
        ```
    - включено проксирование для кластера Kubernetes:
        ```bash
        kubectl proxy
        ```
    - открыта графическая панель Kubernetes по ссылке http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
